[
  {
    "id": "bbf41d1c-ab4c-48fc-82dc-8bae380eca1c",
    "prompt": "<task>You are verifying that every idea/point/concept/argument/detail/url/[[wikilink]]/diagram etc. from the provided notes chunk has been integrated into the updated document body. Use the patch replacements to understand what will be inserted or rewritten in the document. Cross-check the notes against the updated body text as well; if a point already exists in the body, treat it as covered even if no patch replacement references it.</task>\n\n\n\n\n<notes_chunk>\nI think there are two distinct potential issues (am not claiming either are true)\n\n2) this may result in an equilibrium where proposals, instead of stating what they will do to increase tvl if funded, spend more time, or equal time, explaining what they will do to reduce tvl if not funded.\n\nWhether proposals prefer to increase their funded_tvl or decrease their not_funded_tvl I think depends on the value they place on their existing tvl vs how expensive it is to increase their TVL. I expect these to actually be quite close to one another.\n\nability to exit positions early before market closes\n\nmaking trades that move the price closer to the mean have a higher raw roi, and since they move the price less in q space, they have a lower impermanent loss. whereas trades that move the price towards the extremes have a higher impermanent loss, well, actually, no, they don't have a higher impermanent loss. they have like a higher ratio between impermanent loss and the roi. so like basically every percentage of roi moves the price more in q space when you're going to the extremes, therefore you get more impermanent loss\n\nWe want the true forecast to be in the middle of the kpi range, so that ROI of correcting in both directions is similar, but we also want the range to be narrow so that the % ROI of each tvl increment is large\n\nHow to reduce % price impact per tvl increment when moving towards extreme values... Currently it is very high\n\nAlso need to reduce the amount of capital required to move one tvl increment in downwards direction\n\nif it's unprofitable to correct a mispricing in the cfm, then an attacker can simply move whichever market they want to move in order to increase the delta to the unprofitable region, and then it just won't be corrected\n\nSo there are basically three problems:\nCapital requirements\nRoI (ignoring price impact)\nPrice impact\n\nthe way you adjust the up token so that there's a linear relationship between the roi and the kpi adjustment i think is basically like scaling the distribution of the kpi range so that it's parabolic with respect to the price range. so you kind of increase the interval between each kpi increment as a function of how many kpi increments there have already been. but the problem is if you do this with the up token then you're actually making the problem even worse for the down token\n\nSo in a sense the existing linear metric<>payoff mapping is a compromise between an optimal payoff function for UP tokens and an optimal payoff function for DOWN tokens\n\nIf you were to make both the UP and DOWN payoffs optimal, they would no longer sum to 1, hence making the conditional contract much more complex and causing it to take on some kind of exposure to the metric which is quite undesirable\n\nfor cfm-2, i think we should extend the observation window if the recommended allocation changes during the observation window\n\nwe can significantly reduce the price impact of trades that are moving the price towards the extremes by just significantly reducing the liquidity near the extremes so that therefore the weighted average price of such trades is much closer to the starting price than to the final price, therefore causing the effective price impact to be much lower because most of the trade size was executed at a better price than the average price. so this can be solved using uniswap v3\n\nI also just realised that we can significantly reduce the price impact by using univ3 concentrated liquidity.\n\nCounterintuitively, the way we can reduce the price impact of trades which move the price towards extremes is to actually reduce liquidity near extremes.\n\nWe can \"deal with\" the RoI issue by just ensuring that the correct decision is still made even if the market has an \"error margin\" = tvl range * traderHurdleRate\n\nIf we also can expect the forecast to remain near the middle of the TVL range, then this reduces our error margin by 50% because trading is more capital efficient when near the middle of the range. So this would therefore reduce our error margin to just 8m TVL\n\nSo e.g. if the TVL range is 200m, and trader hurdle rate is 8%, then we have to work with an error margin of 16m TVL. I.e. using a cfm needs to still be superior to the next best alternative mechanism despite this error margin.\n\nThe RoI problem can be solved or at least significantly mitigated via the implementation of a liquidation mechanism to e.g. allow someone to mint 1000 UP each worth only 5c, with $100 of collateral, and then sell them for DOWN.\nThis gives them the same DOWN exposure which would currently require them to deposit $1000, hence in increasing their RoI by 10x what it would currently be.\n\nthis reduces our lp exposure significantly\n\ntalk to robin about futaki extortion problem, and also talk to lajare about it next time we call\n</notes_chunk>\n\n\n\n\n<patch_replacements>\n[Patch 1 Replacement]\n- [[non-adversarial metric alignment index]]\n    - Avoid two failure modes:\n        - Withholding positives: deciding not to do X if not funded even though X would have been done if the CFM did not exist.\n        - Inducing negatives: deciding to do X (harmful) if not funded even though X would not have been done if the CFM did not exist.\n    - Equilibrium risk: proposals may spend as much or more effort describing how they would reduce TVL if not funded as they do explaining how they will increase TVL if funded.\n        - Preference to increase funded_TVL versus decrease not_funded_TVL depends on the value placed on existing TVL versus the cost of increasing TVL.\n        - These values are expected to be close to one another.\n\n[Patch 2 Replacement]\n- Market requirements inventory\n    - Exiting efficiently for LPs and traders.\n        - Provide an efficient close-out mechanism at the end of the trading window with minimal price impact so traders do not need exposure during the evaluation period.\n        - Enable early exit before the market closes.\n\n[Patch 3 Replacement]\n- Concentrate liquidity around the proposal\u2019s decision threshold (Uniswap v3-style)\n    - Concentrate liquidity around the estimated decision threshold for each proposal; the threshold may be deterministic or depend on other proposals\u2019 final market prices, but it can usually be estimated upfront.\n    - This does not require a collateral-versus-scalar token design; a scalar-versus-scalar token market can work.\n    - When the expected value is far from 50% of the maximum, a scalar-versus-collateral token market is more capital efficient than scalar-versus-scalar.\n        - The capital efficiency gain is due to distance from 50%, not due to linear AMM constraints.\n    - A Uniswap v3-style concentrated liquidity approach is likely optimal for this use case.\n    - For counterfactual markets, deposit liquidity in each of the two conditional markets centered around the decision threshold relative to the price of the other market.\n    - Set the spread of liquidity in each market as a function of the other market\u2019s volatility; uncertainty about where the decision threshold will land propagates cross-market.\n    - If the decision threshold is fixed, shape the liquidity distribution to reflect the option value with a strike at the threshold; plug in volatility and time remaining to determine the distribution.\n    - With counterfactual thresholds, adjust the distribution by the uncertainty of the threshold\u2019s location derived from the other market\u2019s volatility; combine uncertainties (e.g., multiplicatively).\n    - Allow the price to move toward the decision threshold so traders can communicate via price; do not force a jump to the threshold, which would suppress credible, skin-in-the-game information exchange.\n    - Within each market, start with a flatter, wider distribution and, over time, concentrate it increasingly around the expected threshold as time to resolution shrinks.\n    - We want the liquidity distribution to be proportional, at each price point, to the value of the option that is out of the money by the distance between that price point and the estimated decision threshold.\n    - Prediction markets are biased away from extremes due to:\n        - Error margins that make extreme beliefs harder to support.\n        - The need to hold a large amount of the opposite token to trade at extremes, driven by the minting ratio.\n        - These are two distinct mechanisms.\n    - Reduce liquidity near extremes to lower the weighted-average price impact of large trades that push toward extremes; most of the trade executes near the starting price.\n    - Use Uniswap v3 concentrated liquidity to significantly reduce effective price impact for trades moving toward extremes.\n    - Reduce the percentage price impact per TVL increment near extremes by shaping the distribution to be much thinner in those regions.\n    - Reduce the capital required to move one TVL increment in the downward direction to keep up/down capital requirements more symmetric.\n\n[Patch 4 Replacement]\n- [[improve trader execution quality index]]\n    - Allow specifying max/min acceptable price and enforce that the marginal price of the trade is no worse than this bound; note this differs from enforcing an average price.\n    - Ensure correcting mispricing remains profitable; otherwise attackers can push the price further into regions where correction is unprofitable and deter reversion.\n    - Frame and address three constraints: capital requirements, ROI (ignoring price impact), and price impact.\n\n[Patch 5 Replacement]\n- Frequent batch auctions during the evaluation period\n    - Frequent batch auctions do not increase adverse selection risk for traders compared to AMMs or order books because traders execute at the clearing price, which reflects all information at execution time.\n    - Slow trade mechanisms can impose higher adverse selection risk:\n        - Does slow trade have higher adverse selection risk for traders compared to a frequent batch auction?\n        - Once the auction period is over, does your trade execute at the current AMM price if it is a superior price, or at the original price?\n        - i think it is at the original price... \ud83d\ude2c which means the trader experiences adverse selection.\n        - The trader may only get executed at the original price if it turns out to be worse than the updated market price when the trade executes.\n    - There is a trade-off between continuous pricing and batching:\n        - Continuous markets can overpay for latency edges when only \u201cwithin 10 minutes\u201d freshness is needed.\n        - Waiting to allow information to be priced in increases uncertainty and adverse selection risk for other traders because they do not know the current market price when they trade; this reduces confidence to trade or provide liquidity.\n    - Frequent batch auctions seem like a nice mechanism for trading during the evaluation period; lower adverse selection risk than orderbooks/AMMs.\n    - Ask blob people why they think that the blob AMM is superior to using a frequent batch auction.\n    - https://www.cremieux.xyz/p/high-frequency-trading-is-good/ good article on fba vs orderbook question\n- Payoff function shape for UP/DOWN tokens\n    - A linear metric-to-payoff mapping is a compromise between optimal UP and optimal DOWN payoff functions.\n    - Making the UP payoff \u201clinear ROI per KPI increment\u201d requires a parabolic mapping over the metric range.\n        - Doing so worsens the DOWN payoff function.\n    - Making both UP and DOWN payoff functions \u201coptimal\u201d breaks the UP+DOWN=1 constraint.\n        - The conditional contract then takes on exposure to the metric, which is undesirable and increases design complexity.\n\n[Patch 6 Replacement]\n- Noise floor estimation\n    - Determine the market\u2019s noise floor and ensure targeted effect sizes exceed it.\n    - The noise floor depends on capital costs, risk aversion, and variance.\n    - Treat an \u201cerror margin\u201d \u2248 TVL_range \u00d7 traderHurdleRate; design decisions must remain correct even within this margin.\n    - Centering the true forecast near the middle of the KPI range reduces this error margin by ~50% because trades are more capital efficient near the middle.\n    - Example: TVL range = 200m and trader hurdle rate = 8% \u21d2 error margin \u2248 16m TVL; the CFM must still outperform alternatives despite this margin.\n\n[Patch 7 Replacement]\n- [[trader capital efficiency index]]\n    - Liquidation-assisted leverage for directional exposure:\n        - Allow minting many low-price UP tokens with modest collateral (e.g., mint 1,000 UP at $0.05 with $100 collateral), then selling them for DOWN to obtain equivalent DOWN exposure with ~10x less capital.\n        - Increases ROI for traders by enabling capital-efficient positioning.\n        - Reduces required external LP exposure for the same market depth.\n\n[Patch 8 Replacement]\n- Determine the correct amount of liquidity for a given proposal ask [[cfm amm liquidity incentives]]\n    - How can we incentivize projects to select their own max TVL without them simply choosing an arbitrarily large cap?\n    - How should we compare bias from a high max TVL against the cost of low liquidity induced by a high max TVL?\n    - Projects have some incentive to submit relatively accurate max TVLs because if it is too high it hurts their chances of being accepted.\n        - The misalignment is not eliminated, but it is also not severe.\n        - The mechanism operates by impacting (increasing) their not-funded market more than their funded market.\n    - Liquidity budget and allocation:\n        - Set the liquidity budget to align with the benefit derived from pivotality.\n        - The liquidity budget can be deterministically calculated from the project or proposal budget.\n        - Determine the single total amount you are willing to spend on proposals; from that, derive the optimal allocation between the proposal budget and the liquidity budget.\n    - Conservative max TVL choice:\n        - If we are not targeting long-tail events and care about average effects, a conservative max TVL is acceptable.\n        - A conservative cap significantly increases capital efficiency across AMM choices and increases trade-up percentage returns.\n    - Use a non-zero minimum TVL in the KPI range to improve capital efficiency; make users\u2019 payoff mapping very clear in the UI.\n    - Prefer KPI ranges that keep the true forecast near the middle and are narrow enough that each TVL increment yields a high percentage ROI; this balances incentives to correct in both directions.\n\n[Patch 9 Replacement]\n- [[decision rule accuracy index]]\n    - How can we compare volatility and depth between markets meaningfully?\n        - depth = sample size?\n            - i.e. this is akin to how much data each unit of time worth of price history tells us about reality\n        - If we assume both sides of the market have equal reaction times, then the price will be\n    - Create a decision rule that accounts for volatility when determining whether a proposal makes sense to fund.\n    - [[cfm1 volatility and twap rule]]\n    - Market volatility is only a lower bound for true uncertainty, because a stable price is compatible with a highly uncertain forecast.\n    - If cfm1 is still v volatile during obs period, we can then use this to inform design of cfm2 ()\n    - For cfm-2, extend the observation window if the recommended allocation changes during the observation window.\n    - Ranking-based allocation rule\n        - Only allocate to a proposal if they remain in the top three for >75% of the observation window; this is akin to low flipping frequency but is more aligned.\n        - If we just want to maximise EV, then just taking an average makes sense.\n        - Applying rules like the above only makes sense if we either 1) can delay to increase certainty, 2) have an ROI floor, or 3) have a risk aversion function.\n\n[Patch 10 Replacement]\n- blackmail & bribery prevention\n    - [[anti-blackmail insurance]]\n    - [[futarchy blackmail attack]]\n    - Even asset-price futaki can be blackmailed by existing service providers threatening to withdraw services unless paid; asset-price futaki may be more resistant because observers can anticipate bribe acceptance, limiting price impact.\n    - Discuss the futaki extortion problem with Robin and with Lajare on the next call.\n</patch_replacements>\n\n\n\n\n<updated_body>\n---\ngist_url: https://gist.github.com/5a7433995f63625e90b075e40f63b4b8\nlive: true\n---\n\nGrouping approach: two levels of grouping: parent-level: group by high-level goal i.e. the same type of categories already in the body of the document such as \"trader capital efficiency\" or \"decision rule accuracy\". child-level: sub-headings should be per-mechanism/per-solution i.e. according to which \"idea\"/solution each note relates to.\n\n#index\n#share\n\n\n\n# todo\n- [x] add problems from research backlog\n- [x] create categories of problem types\n    - [x] create index note for each problem\n    - [x] put problem index notes under each problem type\n- [x] review categories and problems from [[non-cost cfm roadmap]] and [[butter ef grant]] also [[tweet & blog ideas]] and research backlog https://www.notion.so/butterd/16157e47719380cab9e9e61d0b582946?v=16157e4771938023a435000c009551ba\n- [x] categorise problems by importance and what they are relevant to\n- [ ] factor large notes into solutions\n- [ ] categorise solutions in each problem index into how promising they are \n- [ ] put solution notes into notes for each problem index\n- [ ] all notes should either be solution/mech ideas or problems\n- [ ] in each problem doc, categorise solutions according to how promising they are\n- [ ] make sure all solutions in problem notes are properly categorised according to priority\n\n\n- [[mech headings template]]\n- Write about and determine why futarchy for conditional funding is inevitable; articulate the vision and what makes it paradigm changing; justify problem-first exploration.\n- Clean up [[tweet & blog ideas]] and categorise into tweet-only and tweet+blog\n\n# CFOs (short term)\n- [[info incentive sourcing index]]\n- [[cf creator risk and effort-mitigation index]]\n- [[macro conditional fund ideas index]]\n- [[unorthodox conditional funds index]]\n- [[ef and dacc CFOs index]]\n- [[proposal curation index]]\n- [[decision rule trust minimisation index]]\n- [[solicitation of LPs index]]\n- [[public goods solution index]]\n- Optimistic acceptance with futarchy veto\n    - For startups, treat futarchy as a backstop/last-resort decision mechanism because internal information transmission is already efficient and team\u2013outsider info asymmetry is high.\n    - Optimistically accept team-submitted proposals unless a futarchy market is created to veto them to reduce friction.\n    - The ratio between liquidity rental cost and the proposer\u2019s stake amount determines the false positive rate where proposals pass even though they would have been vetoed if a market was created.\n    - The proposer\u2019s staked amount must be sufficient to incentivize investigation so spotting a vetoable proposal is +EV.\n    - The challenger should not need to stake; they should pay the cost of creating the market and receive the proposer\u2019s stake if the market agrees with them.\n    - If the challenger is wrong, they pay the liquidity rental cost and get nothing, which is acceptable.\n    - How to solve this without relying on a legal entity: https://x.com/aradtski/status/1930956057053946000\n\n# CFMs (short term)\n## UI & applications\n- [[shadow cfms gtm index]]\n- [[morpho vault risk-rating advisory cfm]]\n- Unruggable ICO launchpad focus\n    - Permissionless launches and liquidity rental are important.\n    - Current focus appears to be on trustless joint ownership over the info aggregation property of futarchy via an unruggable ICO launchpad.\n    - Investors value mechanisms that reduce rug risk; the mechanism is not perfect and relies on external legal system enforcement for control over offchain assets.\n    - Not enough familiarity with the current ICO market to assess how valuable this would be.\n- Information pricing visibility and info-trading interface\n    - Use AI to show whether specific information appears priced in, helping traders see what the market already knows.\n    - Encourage per-proposal Telegram channels to help traders build relationships and trade useful information while internalizing benefits.\n    - Provide an interface for traders to \"trade\" insights using a tit-for-tat Bittorrent-like principle for market intelligence exchange.\n    - Mitigates adverse selection by surfacing what is known and enabling trustable pairwise information exchange similar to benefits of co-location.\n    - According to a discussion with Robin Hanson, there are various solutions:\n        - Traders can talk to each other to gauge what information is already known by other market participants versus what is not.\n        - Traders who sense what is known can find people who have relevant information but are not traders and do not necessarily want to be.\n        - Traders can then pay these people for their information and trade on what they do not think the market has already factored in.\n        - This is equivalent to how hedge funds employ analysts.\n    - Let the AI assistant read the per-proposal Telegram channel to incorporate shared information, with safeguards against prompt-injection attacks.\n    - Add a swap history panel on each market page to show recent trades and who executed them (e.g., address/ENS) for transparency.\n- GTM: Pivot/fork Blockworks\u2019 VC ranking idea\n    - Either as decision markets (eg if x company raises by eoy, which VCs will be announced as investors) or as prediction markets while keeping a 1 v 1 \u201ctournament\u201d format.\n    - Not fully specified yet; there is likely something valuable here from a content perspective.\n- Trader forecast input and automation\n    - Allow users to specify all predictions for all markets they have an opinion on, and the minimum % returns they are willing to accept; automatically determine the most profitable set of trades so the marginal ROI for each trade is equal.\n    - Allow traders to just specify their forecasts; auto-determine whether to buy UP or DOWN based on whether the user's forecast is above or below the market forecast.\n    - Allow users to plus/minus next to the market forecast and show how much it will cost to move the price to that forecast.\n    - Improve the payout estimation slider to allow specifying a forecast directly.\n    - Show the proposal\u2019s implied impact directly on the trading page.\n    - Show price impact in forecast space rather than q space to match how users reason about forecasts.\n    - Create a trader PnL leaderboard; show the ENS of each trader.\n    - Allow people to trade on impact or on the baseline forecast, as long as impact is fairly decoupled from baseline.\n    - Assume that the not funded price is accurate.\n    - Provide a way to express a strong opinion that automatically corrects the price if it moves away from the stated opinion, so the price cannot be manipulated easily during the observation window.\n    - Ensure the size of the impact is not within the error margin of the forecast; otherwise traders have little incentive to price it in.\n    - Address the problem of the difference between proposals being a rounding error; traders had minimal incentive to care about the impact of the proposal itself and more to care about the baseline forecast.\n    - Create a contract that understands specified futarchy trading intents and executes trades when conditions trigger; it only needs access to relevant market data to check conditions; review how other protocols implement intent execution and consult AI for patterns.\n    - Forecasting \u201cimpact\u201d and forecasting the \u201cbaseline\u201d (not-funded) are different skillsets:\n        - Baseline forecasting is a quantitative time-series task using ecosystem trend data.\n        - Impact forecasting is a qualitative analysis task requiring proposal-specific context and expertise.\n        - There are two trader archetypes: quantitative baseline forecasters and qualitative impact forecasters.\n    - Medium term: collect two forecasts instead of two markets from some users:\n        - Ask for \u201cbaseline TVL\u201d (not funded) and for \u201cimpact.\u201d\n        - If a trader provides only an impact forecast (e.g., +30m), translate this into trades that keep the funded market 30m above the not-funded market.\n        - Let traders who forecast impact assume the not-funded market is reasonably priced by others.\n    - Automation of baseline vs funded forecasting:\n        - Use bots to forecast the not-funded baseline market.\n        - Use humans + AI to forecast the funded market\u2019s qualitative impact.\n    - [[tvl forecast analysis prompt]]\n    - Integrate o3 into the app so people can chat to create a forecast, with access to all relevant proposal details.\n- Enterprise vs consumer focus\n    - Decide whether to optimize for attracting new traders per proposal or to serve repeat long-term traders.\n    - Do not plan to manually onboard individual traders; it is too capital intensive.\n    - Make the UI intuitive to reduce onboarding friction.\n- Project incentive transparency\n    - It is difficult to see the actual incentives projects have run on a particular chain; surface this data for traders.\n    - Ask projects for details of all incentive campaigns and data justifying historical performance on different chains.\n    - Collect data on impact duration and time-to-impact to inform forecasts.\n    - Ask projects to write a concise <500w case for their proposal\u2019s impact before market launch.\n- UI feedback and guidance\n    - The guide/help tips often appear when opening the trading page; review trigger conditions.\n    - More UI feedback: https://butterygroup.slack.com/archives/C08PEQBD5MZ/p1752076301235699\n    - Add a time-remaining indicator to each market.\n    - Add a live TWAP display on the site.\n    - Project estimate overlays can make charts hard to read; refine or allow toggling.\n    - Do not show up-to-max TVL KPI on charts; cap charts slightly above the max TVL over the period for readability.\n    - Consider a simple incentive to make a trade so people know they will earn something even if they do not fully understand the market yet; current uncertainty deters trying the product.\n    - Capture and address user confusion such as:\n        - \" So this basically means that if i don't accurately predict whether a project will be funded, i lose all my money? Why should i win or lose money by guessing which project is going to get funding, when i am supposed to guess at their growth in TVL?\" - devansh confusion\n- Telegram bot and notifications\n    - Create a Telegram bot for trading on the CFM.\n    - The bot should post:\n        - Price updates.\n        - Each project\u2019s implied impact.\n        - Estimated profitability from moving the price by x% (i.e., the incentive to correct mispricing).\n- Market requirements inventory\n    - Exiting efficiently for LPs and traders.\n        - Provide an efficient close-out mechanism at the end of the trading window with minimal price impact so traders do not need exposure during the evaluation period.\n        - Enable early exit before the market closes.\n    - Trading in the post-decision window without requiring heavy LPing.\n    - Trading during the trading window without high impermanent loss.\n    - Passively expressing opinions persistently without being active on the site.\n    - Specifying opinions as a range or as a delta between the conditional markets.\n- [[misc cfm ui changes]]\n\n## amm\n- [[amm capital efficiency index]]\n- [[unused lp UP DOWN tokens index]]\n- Token merge mechanics\n    - Auto-merge any UP, DOWN, or IF tokens that are mergeable to reduce fragmentation and user overhead.\n    - Fix the \u201cmerge max outcome tokens\u201d behavior where it chooses the current market\u2019s output token quantity even when it is not the minimum of the two, causing errors.\n    - Investigate and fix a ~30s delay between merging and when balances update, even after several page refreshes.\n- [[lp & trader unwanted kpi exposure index]]\n- [[lp risk reduction index]]\n- [[improve trader execution quality index]]\n    - Allow specifying max/min acceptable price and enforce that the marginal price of the trade is no worse than this bound; note this differs from enforcing an average price.\n    - Ensure correcting mispricing remains profitable; otherwise attackers can push the price further into regions where correction is unprofitable and deter reversion.\n    - Frame and address three constraints: capital requirements, ROI (ignoring price impact), and price impact.\n- [[info price discrimination via liquidity scheduling index]]\n- [[cfm amm liquidity management]]\n    - Use an AMM hook to temporarily increase fees when liquidity is added or when the liquidity distribution is changed (like bunny v3); maybe we can just use that AMM.\n    - Prefer AMMs that support continuous liquidity addition; the resulting arbitrage costs are acceptable.\n    - Be cautious about manipulation induced by liquidity deposits; even a continuous liquidity increase could be subject to manipulation.\n    - Can you add liquidity safely to prediction markets?\n    - Use a high-fee AMM, or a variable-fee AMM, during the evaluation period.\n    - Requirements:\n        - We need to specify the TWAP and the liquidity schedule beyond just the ramp-up; even if solving the ramp-up will likely make the rest easy, make all of these as clear as possible, starting from the bigger picture.\n        - Pull all of this info into the Notion file \u201cliquidity overview\u201d and rename it to \u201cUF CFM strategy\u201d (or similar).\n        - Add to: https://www.notion.so/butterd/CFM-Liquidity-Overview-20057e47719380198a35d9b87d0ec7a1?source=copy_link#21057e47719380e1910dfc9e7f6061e0\n    - Consider allocating less liquidity to the not-funded market if we value the quantitative baseline forecast less than the qualitative funded-market forecast.\n- [[post-decision cfm liquidity incentives]]\n- LP incentive efficiency\n    - It is inefficient to pay external LPs to provide liquidity irrespective of realized costs; stop using Merkle-based distributions until we can pay only for the actual costs incurred.\n- [[post-trading window liquidity management plan]]\n    - Keep liquidity after the observation window low enough to incentivize pricing before the window ends.\n- [[UP DOWN aware liq management spec]]\n- [[cfm1 liquidity management strategy index]]\n- [[cfm twap volatility ceiling]]\n- TWAP function and parameters\n    - TWAP function\n        - Sampling cadence \u2013 Record a spot price once every minute\n        - Price is measured in p space, to avoid infinities\n        - Per-sample movement limit \u2013 Let \u0394t be the time since the previous sample in hours. Compute a price-change budget limit = 0.15 \u00b7 \u0394t \u00b7 lastSpotPrice. The raw spot price S is clipped to the band [lastSpotPrice \u2212 limit, lastSpotPrice + limit]; call the clipped value Pcap.\n        - Running TWAP update \u2013 Append Pcap to the observation window and drop the oldest entry when the window length is reached; the TWAP at any moment is the arithmetic mean of the values now in the window.\n        - Initial price \u2013 At deployment seed prevTWAP with a configurable default\n        - Optional hardening \u2013 jitter the exact sampling timestamp randomly within each minute to deny manipulators a deterministic target\n    - TWAP params\n        - minimum sample frequency: 1 minute\n        - volatility ceiling per hour: 15%\n        - twap warm-up safety buffer: 2\n            - so that twap input price can reach market price before beginning of observation window, so that starting price doesn't impact twap calculations\n        - maxAssumedRatioBetweenMarketPriceAndMidPrice: 20 (i.e. equiv to 95%)\n        - twap warm-up period (hours)\n            - such that twap warm-up period * volatility ceiling * twap warm-up safety buffer = maxAssumedRatioBetweenMarketPriceAndMidPrice\n            - twap warm-up period = maxAssumedRatioBetweenMarketPriceAndMidPrice / (volatility ceiling * twap warm-up safety buffer)\n- [[cfm liquidity ramping]]\n    - Decrease the spread of liquidity as a function of time until resolution; with less time remaining, the chance the price traverses to the decision threshold is lower.\n    - Increase total liquidity over time at a rate proportional to current pivotality scaled by the percentage progress through the market period.\n        - If the market becomes less pivotal, stop depositing additional liquidity or withdraw previously added liquidity.\n    - Schedule liquidity between the two counterfactual markets: start flat and low, then increase over the duration; increase faster on more pivotal markets and slower on less pivotal ones.\n    - Apply the same time-based schedule within each market when using Uniswap v3: start relatively flat, then make the distribution increasingly concentrated around the expected threshold over time.\n    - Should we dynamically adjust liquidity rather than a pure linear ramp-up, or does this create too much uncertainty for traders about whether they will be able to profit from their research?\n    - Balancer pools apparently support continuous liquidity ramping. https://docs.balancer.fi/concepts/explore-available-balancer-pools/gyroscope-pool/gyro-2clp.html#higher-fees-for-lps https://docs-v2.balancer.fi/concepts/pools/managed.html#gradual-updates\n    - Observation-window liquidity adjustments:\n        - Reducing liquidity during the observation window can help if traders are aware in advance and understand the implications:\n            - It distinguishes manipulators from informed traders by incentivizing earlier informed trades.\n            - It encourages information to be priced in before the observation window, reducing volatility during the window.\n        - If these conditions are not met (e.g., in cfm1), reducing liquidity by default likely makes manipulation easier and is not worthwhile.\n    - Near the end of the market, incentives to correct the price weaken if liquidity is not increasing; traders can profit only by holding through resolution or if others subsequently move the price in the same direction.\n- Concentrate liquidity around the proposal\u2019s decision threshold (Uniswap v3-style)\n    - Concentrate liquidity around the estimated decision threshold for each proposal; the threshold may be deterministic or depend on other proposals\u2019 final market prices, but it can usually be estimated upfront.\n    - This does not require a collateral-versus-scalar token design; a scalar-versus-scalar token market can work.\n    - When the expected value is far from 50% of the maximum, a scalar-versus-collateral token market is more capital efficient than scalar-versus-scalar.\n        - The capital efficiency gain is due to distance from 50%, not due to linear AMM constraints.\n    - A Uniswap v3-style concentrated liquidity approach is likely optimal for this use case.\n    - For counterfactual markets, deposit liquidity in each of the two conditional markets centered around the decision threshold relative to the price of the other market.\n    - Set the spread of liquidity in each market as a function of the other market\u2019s volatility; uncertainty about where the decision threshold will land propagates cross-market.\n    - If the decision threshold is fixed, shape the liquidity distribution to reflect the option value with a strike at the threshold; plug in volatility and time remaining to determine the distribution.\n    - With counterfactual thresholds, adjust the distribution by the uncertainty of the threshold\u2019s location derived from the other market\u2019s volatility; combine uncertainties (e.g., multiplicatively).\n    - Allow the price to move toward the decision threshold so traders can communicate via price; do not force a jump to the threshold, which would suppress credible, skin-in-the-game information exchange.\n    - Within each market, start with a flatter, wider distribution and, over time, concentrate it increasingly around the expected threshold as time to resolution shrinks.\n    - We want the liquidity distribution to be proportional, at each price point, to the value of the option that is out of the money by the distance between that price point and the estimated decision threshold.\n    - Prediction markets are biased away from extremes due to:\n        - Error margins that make extreme beliefs harder to support.\n        - The need to hold a large amount of the opposite token to trade at extremes, driven by the minting ratio.\n        - These are two distinct mechanisms.\n    - Reduce liquidity near extremes to lower the weighted-average price impact of large trades that push toward extremes; most of the trade executes near the starting price.\n    - Use Uniswap v3 concentrated liquidity to significantly reduce effective price impact for trades moving toward extremes.\n    - Reduce the percentage price impact per TVL increment near extremes by shaping the distribution to be much thinner in those regions.\n    - Reduce the capital required to move one TVL increment in the downward direction to keep up/down capital requirements more symmetric.\n- Frequent batch auctions during the evaluation period\n    - Frequent batch auctions do not increase adverse selection risk for traders compared to AMMs or order books because traders execute at the clearing price, which reflects all information at execution time.\n    - Slow trade mechanisms can impose higher adverse selection risk:\n        - Does slow trade have higher adverse selection risk for traders compared to a frequent batch auction?\n        - Once the auction period is over, does your trade execute at the current AMM price if it is a superior price, or at the original price?\n        - i think it is at the original price... \ud83d\ude2c which means the trader experiences adverse selection.\n        - The trader may only get executed at the original price if it turns out to be worse than the updated market price when the trade executes.\n    - There is a trade-off between continuous pricing and batching:\n        - Continuous markets can overpay for latency edges when only \u201cwithin 10 minutes\u201d freshness is needed.\n        - Waiting to allow information to be priced in increases uncertainty and adverse selection risk for other traders because they do not know the current market price when they trade; this reduces confidence to trade or provide liquidity.\n    - Frequent batch auctions seem like a nice mechanism for trading during the evaluation period; lower adverse selection risk than orderbooks/AMMs.\n    - Ask blob people why they think that the blob AMM is superior to using a frequent batch auction.\n    - https://www.cremieux.xyz/p/high-frequency-trading-is-good/ good article on fba vs orderbook question\n- Payoff function shape for UP/DOWN tokens\n    - A linear metric-to-payoff mapping is a compromise between optimal UP and optimal DOWN payoff functions.\n    - Making the UP payoff \u201clinear ROI per KPI increment\u201d requires a parabolic mapping over the metric range.\n        - Doing so worsens the DOWN payoff function.\n    - Making both UP and DOWN payoff functions \u201coptimal\u201d breaks the UP+DOWN=1 constraint.\n        - The conditional contract then takes on exposure to the metric, which is undesirable and increases design complexity.\n- Determine the correct amount of liquidity for a given proposal ask [[cfm amm liquidity incentives]]\n    - How can we incentivize projects to select their own max TVL without them simply choosing an arbitrarily large cap?\n    - How should we compare bias from a high max TVL against the cost of low liquidity induced by a high max TVL?\n    - Projects have some incentive to submit relatively accurate max TVLs because if it is too high it hurts their chances of being accepted.\n        - The misalignment is not eliminated, but it is also not severe.\n        - The mechanism operates by impacting (increasing) their not-funded market more than their funded market.\n    - Liquidity budget and allocation:\n        - Set the liquidity budget to align with the benefit derived from pivotality.\n        - The liquidity budget can be deterministically calculated from the project or proposal budget.\n        - Determine the single total amount you are willing to spend on proposals; from that, derive the optimal allocation between the proposal budget and the liquidity budget.\n    - Conservative max TVL choice:\n        - If we are not targeting long-tail events and care about average effects, a conservative max TVL is acceptable.\n        - A conservative cap significantly increases capital efficiency across AMM choices and increases trade-up percentage returns.\n    - Use a non-zero minimum TVL in the KPI range to improve capital efficiency; make users\u2019 payoff mapping very clear in the UI.\n    - Prefer KPI ranges that keep the true forecast near the middle and are narrow enough that each TVL increment yields a high percentage ROI; this balances incentives to correct in both directions.\n\n## metric\n- [[metric manipulation index]]\n    - No one with an interest in a particular proposal winning over another should be able to decrease the metric, with plausible deniability as to why they are doing so, at a lower cost than the benefit their proposal derives from doing so.\n        - Expected cost of decreasing the metric = cost of doing so * probability of not being funded, despite engaging in this attack.\n        - It still needs to be unprofitable to decrease the metric even when the cost is multiplied by p(not funded).\n    - Distinguish two risks:\n        - Structural advantage: a single project may be able to guarantee a win if it can exploit the vector better than others.\n            - Large projects may credibly decrease their TVL more if not funded and can allocate more capital to short their not-funded markets than smaller projects.\n        - Plausible deniability: attacks that maintain plausible deniability are hard to penalize or deter.\n- [[non-adversarial metric alignment index]]\n    - Avoid two failure modes:\n        - Withholding positives: deciding not to do X if not funded even though X would have been done if the CFM did not exist.\n        - Inducing negatives: deciding to do X (harmful) if not funded even though X would not have been done if the CFM did not exist.\n    - Equilibrium risk: proposals may spend as much or more effort describing how they would reduce TVL if not funded as they do explaining how they will increase TVL if funded.\n        - Preference to increase funded_TVL versus decrease not_funded_TVL depends on the value placed on existing TVL versus the cost of increasing TVL.\n        - These values are expected to be close to one another.\n- Metrics to track for evaluation\n    - Use net inflows as the primary KPI instead of raw change because net inflows already account for price changes.\n        - Price inflows at the time they are deposited so asset price changes do not retroactively reprice existing assets; only new inflows are affected, improving predictability.\n    - Track the percentage of traders able to close positions before the end of the trading window.\n    - Divide or control for price in futarchy metrics to reduce variance and increase accountability when evaluating predictions.\n    - Define the concept of attributability in the metric considerations documents.\n- Noise floor estimation\n    - Determine the market\u2019s noise floor and ensure targeted effect sizes exceed it.\n    - The noise floor depends on capital costs, risk aversion, and variance.\n    - Treat an \u201cerror margin\u201d \u2248 TVL_range \u00d7 traderHurdleRate; design decisions must remain correct even within this margin.\n    - Centering the true forecast near the middle of the KPI range reduces this error margin by ~50% because trades are more capital efficient near the middle.\n    - Example: TVL range = 200m and trader hurdle rate = 8% \u21d2 error margin \u2248 16m TVL; the CFM must still outperform alternatives despite this margin.\n- Uncertainty magnitude vs impact\n    - Metrics uncertainty may be much larger than the proposal\u2019s impact; discuss this with Robin Hanson.\n\n## costs\n- [[trader capital efficiency index]]\n    - Liquidation-assisted leverage for directional exposure:\n        - Allow minting many low-price UP tokens with modest collateral (e.g., mint 1,000 UP at $0.05 with $100 collateral), then selling them for DOWN to obtain equivalent DOWN exposure with ~10x less capital.\n        - Increases ROI for traders by enabling capital-efficient positioning.\n        - Reduces required external LP exposure for the same market depth.\n- Problem of low RoIs: https://docs.google.com/spreadsheets/d/1qGRTQA7q7pztfJBwnx0kuAoALTxFroTvJmhxBPBGjrA/edit?gid=0#gid=0\n- [[info incentive sizing index]]\n    - Pay only as much as needed for information via an auction: increase payments until the marginal cost per percentage point of accuracy exceeds the marginal value of that information.\n    - This enables precise optimization of the information budget and is significant for cost-effectiveness.\n- [[info cost reduction index]]\n    - Explore minting bond-type prediction market shares to elicit information without inaccuracy induced by traders\u2019 opportunity costs.\n\n## misc.\n- [[non-adversarial decision selection bias index]]\n    - Need to solve non adversarial dsb if we are to calculate roi; re-analyse rh's article on this; maybe just only resolve once price is stable\n    - The target is the delta between TVL(cfm doesn\u2019t exist) and TVL(CFM exists and proposal receives funding).\n    - A CFM cannot directly measure the \u201ccfm doesn\u2019t exist\u201d counterfactual; acknowledge this limitation when designing evaluation methods.\n- [[fundee accountability index]]\n- [[decision rule accuracy index]]\n    - How can we compare volatility and depth between markets meaningfully?\n        - depth = sample size?\n            - i.e. this is akin to how much data each unit of time worth of price history tells us about reality\n        - If we assume both sides of the market have equal reaction times, then the price will be\n    - Create a decision rule that accounts for volatility when determining whether a proposal makes sense to fund.\n    - [[cfm1 volatility and twap rule]]\n    - Market volatility is only a lower bound for true uncertainty, because a stable price is compatible with a highly uncertain forecast.\n    - If cfm1 is still v volatile during obs period, we can then use this to inform design of cfm2 ()\n    - For cfm-2, extend the observation window if the recommended allocation changes during the observation window.\n    - Ranking-based allocation rule\n        - Only allocate to a proposal if they remain in the top three for >75% of the observation window; this is akin to low flipping frequency but is more aligned.\n        - If we just want to maximise EV, then just taking an average makes sense.\n        - Applying rules like the above only makes sense if we either 1) can delay to increase certainty, 2) have an ROI floor, or 3) have a risk aversion function.\n- [[empirical accuracy measurement index]]\n    - Build a statistical model to analyze market behavior during the observation period and estimate the probability that observed differences are noise versus true effects.\n- Support for non-counterfactual markets when counterfactual structure is unnecessary.\n- How to create pm for highly improbable long term binary event not events...\n- Decision theory and futarchy\n    - if you could implement a futarchy which employs functional decision theory without needing a trusted third-party oracle as the commitment mechanism, then you can use futarchy to create a trustless oracle\n    - more to the point, an oracle that is not susceptible to retirement attacks, given that retirement attacks kind of aren't compatible with fdt, or at least they're like much less likely under fdt than they are under cdt\n    - Does futarchy implement causal decision theory?\n    - Is it possible to make a futarchy that implements non-causal decision theory so it can cooperate with other futarchies more efficiently?\n    - Using non-causal decision theory, such as functional or timeless decision theory, only makes sense if you think that there are copies of you.\n        - Otherwise, you should always use causal decision theory and only act as though you're using, say, functional decision theory to the extent that you believe that other people are modeling you and that your decisions will impact how people interact with you in the future.\n        - Also act as though you use functional decision theory to the extent that you cannot make decisions without people predicting your decisions and therefore treating you differently.\n        - Beyond those cases, you should always use causal decision theory.\n    - A heuristic for choosing:\n        - > \u201cAsk: What, exactly, would change in the world if I wrote a different line of code in my brain? Track only those channels.\u201d\n        - If the answer is just ordinary physics \u21d2 CDT.\n        - If the answer includes knowing I chose X means the hidden coin is tails \u21d2 EDT.\n        - If the answer includes every copy or predictor of my algorithm outputs X \u21d2 FDT / UDT.\n        - If you cannot even assign a probability to any of that \u21d2 go for a regret-based rule.\n    - Prior note claiming FDT is not resistant to blackmail was incorrect; FDT is resistant to blackmail similarly to updateless decision theory.\n    - The anti-blackmail insurance or bond idea is basically a crypto-economic implementation of updateless decision theory.\n- Cooperate is only rational in Prisoner's Dilemma if you believe the other agent is running the same decision theory as you; mere simulation access does not incentivize them to cooperate.\n- Futaki vs decision theories:\n    - Futaki is always implementing causal decision theory; it may only approximate functional decision theory in certain designs (e.g., asset futaki).\n    - share price futarchy is a bit more like fdt, whereas metric futarchy is a bit more like cdt.\n    - Futaki may approximate functional decision theory using commitments to bind its future self to take actions; commitments are not sufficient because you cannot always know in advance what you should have committed to.\n    - Would a futaki cooperate with itself in prisoner's dilemma, and if not, how can you make one do so?\n\n# cfm deployments\n\n\n# CFOs (longer term)\n- [[cfo deposits & withdrawals index]]\n- [[funding amount optimisation index]]\n    - Implement a descending hurdle rate for ROI:\n        - Start with a high minimum acceptable ROI and decrease it gradually over time.\n        - Fund proposals whose estimated ROI clears the current hurdle; after funding, increase the hurdle by a step to reflect revealed supply at that \"price.\"\n        - This behaves like an auction or TWAMM-style market maker for capital allocation.\n\n\n# CFMs (longer term)\n- [[pass vs fail impact delta maximisation index]]\n- [[info to kpi estimate mapping problem]]\n- [[proposal max-value extraction index]]\n- [[butter network effects index]]\n    - Holding a large, diversified portfolio of market positions is a moat: it offsets idiosyncratic risk and enables more effective diversification than smaller participants.\n- [[adversarial decision selection bias index]]\n- [[invalid outcomes index]]\n- [[futarchy vs tokenised retro funding]]\n    - Futaki is more permissionless than retroactive funding if an attributable metric is not available.\n        - Without an attributable metric, people can free ride on the impact of other proposals because impact cannot be attributed to a particular proposal.\n        - Therefore you cannot permissionlessly accept proposals; you have to use permissions to determine which proposals can participate, making it more closed.\n- [[holding cost-induced sensitivity floor index]]\n- [[capital efficiency asymmetry manipulation index]]\n- [[cfm adverse selection manipulation index]]\n    - Mitigate adverse selection by improving shared knowledge of what is known and priced in, via AI priced-in indicators, per-proposal communication channels, and tit-for-tat information-exchange interfaces (see UI & applications).\n    - High-dimensional LMSR markets allow participants to express opinions on very specific conditional outcomes, reducing adverse selection by letting traders specialize in a single outcome rather than implicitly speculating on many factors; this enables more efficient information incorporation than pure prediction markets.\n- [[cfm privacy index]]\n- [[uncertainty elicitation index]]\n- [[less important cfm manipulation risks index]]\n- [[futarchy commitment mechanisms index]]\n    - Mechanisms to commit to not making certain decisions over specified periods, or to cede control temporarily to another entity without vetoing decisions.\n    - Commitments to not violate agreements/contracts over a period, even if it later becomes favorable to do so.\n    - Determine how to implement such commitments effectively.\n    - CFOs need a commitment mechanism to avoid funding the same or similar proposals within the metric\u2019s observation window; otherwise impact measurement is confounded.\n        - If the metric is price, offset subsequent funding by a calibrated amount because price immediately incorporates new information.\n\n\n# infofi (non-butter)\n- [[explanation elicitation index]]\n    - Create periphery prediction markets on sub-events so traders can use these probabilities to condition and improve the main market\u2019s accuracy.\n    - Enable traders to pay contributors for useful inputs and arguments to incorporate into their models; this is basically like the glea mechanism.\n    - Conditional markets help elicit explanations by revealing the causal (or correlational) impact of X on Y.\n- [[non-cf infofi applications index]]\n- [[non-cf futarchy applications index]]\n- [[analysis and comparison of mech primitives index]]\n- [[secure futarchic oracle index]]\n- [[novel idea elicitation index]]\n- [[bad idea detection index]]\n- [[personal futarchy oracle problem index]]\n- [[personal futarchy applications index]]\n- [[criteria for when ai infofinance is useful]]\n\n\n# non-butter non-infofi\n- [[public goods funding index]]\n- blackmail & bribery prevention\n    - [[anti-blackmail insurance]]\n    - [[futarchy blackmail attack]]\n    - Even asset-price futaki can be blackmailed by existing service providers threatening to withdraw services unless paid; asset-price futaki may be more resistant because observers can anticipate bribe acceptance, limiting price impact.\n    - Discuss the futaki extortion problem with Robin and with Lajare on the next call.\n- [[lottery courts]]\n\n\n# archived\n- [[DAC research]]\n- [[adsb index]]\n- [[adversarial cfm metric design]]\n- [[adversarial determinants of cfm accuracy]]\n- [[asset futarchy proposal value internalisation]]\n- [[bootstrapped kpi futarchy]]\n- [[capital efficiency asymmetry manipulation]]\n- [[cfm adverse selection manipulation]]\n- [[cfm ai bot design]]\n- [[cfm amm liquidity incentives]]\n- [[cfm auction mech design]]\n- [[cfm confounding vectors]]\n- [[pm vs non-market prediction aggregation]]\n- [[cfm decision rule design]]\n- [[cfm metric evaluation criteria]]\n- [[cfm misaligned liquidity distribution solutions]]\n- [[cfm narrative exposure vaults]]\n- [[cfm re-architecture]]\n- [[cfm transfer problem mitigation]]\n- [[cfm twamm or limit orders]]\n- [[concentrate cfm amm liquidity to reflect non-linear importance of kpi range]]\n- [[counterfactual cfm designs]]\n- [[counterfactual cfm rationale]]\n- [[decisive arguments and prediction markets]]\n- [[discussion with zack re: futarchy problems]]\n- [[display historical or baseline kpi for each proposal]]\n- [[dominant assurance contract convo w zack]]\n- [[dsb-optimised adversarial proposal construction]]\n- [[ef and dacc cfm metrics]]\n- [[escrow funds which are granted by cfm]]\n- [[experimentally demo cfm info aggregation superiority]]\n- [[forecast perp]]\n- [[uf cfm 1 index]]\n- [[futarchy as singleton risk]]\n- [[futarchy beauty contest dynamics]]\n- [[futarchy goal drift problem]]\n- [[futarchy proposal finality problem]]\n- [[glia truth alignment mechanisms]]\n- [[hybrid futarchy designs]]\n- [[if-token markets]]\n- [[inapplicability of pm-amm to cfms]]\n- [[infofi privacy mechanisms]]\n- [[infofi terminology distinctions]]\n- [[initial futarchy use cases]]\n- [[jury cfm]]\n- [[metric-neutral cfm positions]]\n- [[non-adversarial cfm metric design]]\n- [[non-adversarial determinants of cfm accuracy]]\n- [[ongoing funding markets]]\n- [[parameter-optimisation markets]]\n- [[personal futarchy for prioritising reading material]]\n- [[pivotal funding mech]]\n- [[pm for highly improbable events]]\n- [[pm volatility & ai-curated breaking news web app]]\n- [[potential sources of costs unique to cfms]]\n- [[prediction market leverage index]]\n- [[prediction market uncertainty elicitation]]\n- [[recycled cfm considerations]]\n- [[resolution period exposure problem index]]\n- [[retroactive cfms]]\n- [[reversible swap decision markets]]\n- [[review and propose optimism cfm kpi]]\n- [[self-resolving cfms]]\n- [[solving the invalid problem in futarchy design]]\n- [[tentative conditional prediction markets]]\n- [[threaded social web annotation]]\n- [[trade-offs of retro vs prior funding]]\n- [[vc and various other cfos]]\n- [[yield-bearing collateral support]]\n\n\n# Two aspects to public goods\n- Pivotality\n- having enough exposure to externalities\n\n- EIP-2025 proposal to send some block subsidies to an address: https://github.com/ethereum/EIPs/blob/master/EIPS/eip-2025.md\n- FDAC explainer: https://github.com/zack-bitcoin/amoveo-docs/blob/master/use-cases-and-ideas/dominant_assurance_contract.md\n- DAC explainer: https://en.bitcoin.it/wiki/Dominant_Assurance_Contracts\n\n- add ai summary of proposal details to market page\n- allowing funders to only allocate capital to proposals which pass a roi threshold: reduces their risk\n</updated_body>\n\n\n\n\n<response_guidelines>\nReport whether any note content is missing or materially altered. Respond with a concise single paragraph beginning with 'OK -' if everything is covered or 'MISSING -' followed by details of any omissions. Seperate each omission by two newlines and for each one, provide the following:\n    Notes:\"...\"\n    Body:\"...\"\n    Explanation: \"...\"\nQuote the exact text from the notes chunk containing the missing detail and quote the exact passage from the updated document body that should cover it (or state Body:\"<not present>\" if nothing is relevant). Explain precisely what information is still missing or altered without omitting any nuance.\n</response_guidelines>",
    "context_label": "chunk 2/22",
    "chunk_index": 1,
    "total_chunks": 22
  }
]